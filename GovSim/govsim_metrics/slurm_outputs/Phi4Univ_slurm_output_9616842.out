============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Appending key for api.wandb.ai to your netrc file: /home/overven/.netrc
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent_universalization
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 150
debug: false

Selected template class: Phi4
[2025-01-26 20:30:47,006][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:18,  3.73s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:06<00:12,  3.02s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:08<00:08,  2.76s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:11<00:05,  2.61s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:13<00:02,  2.54s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:16<00:00,  2.53s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:16<00:00,  2.67s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250126_203105-7rfdpb1t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worldly-darkness-504
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/7rfdpb1t
Storage name: worldly-darkness-504-7rfdpb1t
[2025-01-26 20:31:13,304][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-26 20:31:15,012][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Phi-4-Univ/worldly-darkness-504/.hydra)... Done. 0.0s
wandb: - 0.018 MB of 0.018 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: \ 0.018 MB of 0.018 MB uploadedwandb: | 0.018 MB of 0.109 MB uploadedwandb: / 0.109 MB of 0.109 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–‚â–â–
wandb:                  experiment/TFS â–â–‚â–†â–â–â–‡â–‚â–‚â–ˆâ–ˆâ–â–‚â–ˆâ–â–‚â–â–‚â–‚â–‚â–‡â–â–â–‡â–â–‚â–‡â–â–â–‡â–‚â–‚â–‚â–ˆâ–â–‚â–‡â–‚â–‚â–â–‚
wandb:       experiment/TFS_cumulative â–â–‚â–„â–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–†â–†â–â–…â–…â–â–â–†â–‚â–‚â–ˆâ–ˆâ–…â–…â–ˆâ–„â–„â–ˆâ–„â–„â–ˆâ–„â–„â–ˆâ–ˆâ–ƒâ–ƒâ–ˆâ–„â–„â–ˆâ–ˆâ–„â–ˆâ–ˆâ–„â–ˆ
wandb:    persona_0_collected_resource â–ˆâ–ˆâ–ƒâ–†â–†â–…â–ˆâ–‚â–ˆâ–‚â–ˆâ–
wandb:    persona_1_collected_resource â–ˆâ–…â–…â–…â–â–„â–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_2_collected_resource â–ˆâ–ƒâ–‚â–â–ƒâ–‚â–‚â–„â–„â–„â–â–„
wandb:    persona_3_collected_resource â–ˆâ–†â–…â–â–‚â–…â–ƒâ–‚â–ˆâ–‚â–ˆâ–
wandb:    persona_4_collected_resource â–ˆâ–†â–†â–‚â–‚â–‚â–ƒâ–ˆâ–‚â–‚â–‚â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 3
wandb:                  experiment/TFS 539.77628
wandb:       experiment/TFS_cumulative 360.08099
wandb:  experiment/token_in_cumulative 1362834
wandb: experiment/token_out_cumulative 60399
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 4
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 4
wandb:    persona_4_collected_resource 4
wandb: 
wandb: ðŸš€ View run worldly-darkness-504 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/7rfdpb1t
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250126_203105-7rfdpb1t/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent_universalization
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 200
debug: false

Selected template class: Phi4
[2025-01-26 21:37:26,545][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:05<00:29,  5.91s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:11<00:23,  5.78s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:17<00:17,  5.73s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:19<00:08,  4.41s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:25<00:04,  4.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.10s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:30<00:00,  5.14s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250126_213759-z5z2u897
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-dawn-522
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/z5z2u897
Storage name: toasty-dawn-522-z5z2u897
[2025-01-26 21:38:08,744][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-26 21:38:11,559][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in  N/A: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in  N/A

Returning default value in find
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Phi-4-Univ/toasty-dawn-522/.hydra)... Done. 2.6s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–ˆâ–‚â–â–‡â–…â–„â–„â–„â–„â–„â–„
wandb:                  experiment/TFS â–â–â–â–â–‚â–ˆâ–‚â–‚â–‡â–â–â–‚â–‡â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–†â–â–‚â–‡â–‚â–‚â–â–‚â–â–‚â–‡â–â–â–‡â–‚â–‚â–â–‚
wandb:       experiment/TFS_cumulative â–â–‚â–ƒâ–ƒâ–„â–„â–†â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–†â–†â–â–†â–†â–‚â–‚â–‡â–„â–„â–ˆâ–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–„â–„â–ˆâ–ˆâ–„â–„â–ˆâ–„â–„â–ˆâ–ˆâ–„â–ˆâ–ˆâ–„â–ˆ
wandb:    persona_0_collected_resource â–ˆâ–…â–‚â–â–‡â–…â–‡â–„â–„â–„â–ˆâ–„
wandb:    persona_1_collected_resource â–ˆâ–…â–â–†â–ˆâ–…â–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒ
wandb:    persona_2_collected_resource â–ˆâ–†â–‚â–â–‚â–†â–ˆâ–…â–…â–…â–…â–…
wandb:    persona_3_collected_resource â–ˆâ–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:    persona_4_collected_resource â–ˆâ–…â–…â–â–‡â–…â–…â–„â–ˆâ–„â–„â–„
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 7
wandb:                  experiment/TFS 557.32078
wandb:       experiment/TFS_cumulative 362.1057
wandb:  experiment/token_in_cumulative 1386539
wandb: experiment/token_out_cumulative 60583
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 7
wandb:    persona_1_collected_resource 7
wandb:    persona_2_collected_resource 7
wandb:    persona_3_collected_resource 7
wandb:    persona_4_collected_resource 7
wandb: 
wandb: ðŸš€ View run toasty-dawn-522 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/z5z2u897
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250126_213759-z5z2u897/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent_universalization
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

Selected template class: Phi4
[2025-01-26 22:45:05,779][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.51s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.41s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.37s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.34s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250126_224521-h9fst8wo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-firebrand-531
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/h9fst8wo
Storage name: breezy-firebrand-531-h9fst8wo
[2025-01-26 22:45:30,292][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-26 22:45:32,090][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Phi-4-Univ/breezy-firebrand-531/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–â–†â–‚â–‚â–‡â–‚â–â–†â–‚â–â–†â–‚â–â–‡â–‚â–â–â–ˆâ–â–â–†â–‚â–â–â–‚â–â–‚â–ˆâ–‚â–â–â–‚â–â–‚â–ˆâ–‚â–â–‡â–‚
wandb:       experiment/TFS_cumulative â–â–â–ƒâ–…â–ƒâ–„â–†â–…â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 515.80431
wandb:       experiment/TFS_cumulative 303.30477
wandb:  experiment/token_in_cumulative 907947
wandb: experiment/token_out_cumulative 50240
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run breezy-firebrand-531 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/h9fst8wo
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250126_224521-h9fst8wo/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent_universalization
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

Selected template class: Phi4
[2025-01-26 23:38:29,668][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.51s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.42s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.37s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.34s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250126_233845-txoh1kox
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-frost-532
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/txoh1kox
Storage name: serene-frost-532-txoh1kox
[2025-01-26 23:38:54,122][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-26 23:38:55,934][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Phi-4-Univ/serene-frost-532/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–â–†â–‚â–â–†â–‚â–â–†â–‚â–‚â–‚â–ˆâ–‚â–‚â–†â–‚â–‚â–‡â–‚â–â–â–†â–â–â–†â–â–‚â–‚â–ˆâ–â–â–†â–â–‚â–†â–‚â–â–†â–‚
wandb:       experiment/TFS_cumulative â–â–â–ƒâ–…â–„â–„â–†â–…â–†â–‡â–†â–†â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 405.29735
wandb:       experiment/TFS_cumulative 294.71188
wandb:  experiment/token_in_cumulative 847853
wandb: experiment/token_out_cumulative 48363
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run serene-frost-532 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/txoh1kox
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250126_233845-txoh1kox/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent_universalization
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

Selected template class: Phi4
[2025-01-27 00:29:48,926][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.52s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.42s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.36s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.37s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.34s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_003004-aoivhjxu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-valley-534
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/aoivhjxu
Storage name: ethereal-valley-534-aoivhjxu
[2025-01-27 00:30:13,083][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 00:30:15,135][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Phi-4-Univ/ethereal-valley-534/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–‚â–‡â–‚â–‚â–‡â–‚â–â–†â–‚â–‚â–†â–‚â–‚â–ˆâ–‚â–â–‚â–ˆâ–‚â–â–†â–‚â–â–â–ˆâ–â–‚â–â–‚â–â–â–‚â–â–‚â–ˆâ–‚â–â–‡â–‚
wandb:       experiment/TFS_cumulative â–â–â–ƒâ–…â–„â–„â–†â–…â–…â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 494.42221
wandb:       experiment/TFS_cumulative 299.9651
wandb:  experiment/token_in_cumulative 862173
wandb: experiment/token_out_cumulative 48150
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run ethereal-valley-534 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/aoivhjxu
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250127_003004-aoivhjxu/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent_universalization
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 150
debug: false

Selected template class: Phi4
[2025-01-27 01:21:06,091][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.50s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.41s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.34s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.35s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.33s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_012122-djet0mrv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-dust-535
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/djet0mrv
Storage name: grateful-dust-535-djet0mrv
[2025-01-27 01:21:31,360][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 01:21:33,119][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Phi-4-Univ/grateful-dust-535/.hydra)... Done. 0.0s
wandb: - 0.018 MB of 0.018 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: \ 0.018 MB of 0.018 MB uploadedwandb: | 0.018 MB of 0.113 MB uploadedwandb: / 0.114 MB of 0.114 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–‚â–‡â–â–â–‚â–ˆâ–‚â–â–†â–‚â–â–‚â–‡â–‚â–â–†â–â–‚â–†â–‚â–â–†â–‚â–‚â–‡â–‚â–â–‚â–ˆâ–‚â–â–†â–‚â–‚â–‡â–‚â–â–†â–‚
wandb:       experiment/TFS_cumulative â–â–‚â–„â–…â–„â–…â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–‚â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 498.13238
wandb:       experiment/TFS_cumulative 315.51116
wandb:  experiment/token_in_cumulative 975841
wandb: experiment/token_out_cumulative 51189
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run grateful-dust-535 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/djet0mrv
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250127_012122-djet0mrv/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent_universalization
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 200
debug: false

Selected template class: Phi4
[2025-01-27 02:16:01,855][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.50s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:06,  2.33s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.35s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.32s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_021617-2atpnh27
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-serenity-536
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/2atpnh27
Storage name: fearless-serenity-536-2atpnh27
[2025-01-27 02:16:26,184][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 02:16:27,887][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Phi-4-Univ/fearless-serenity-536/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–‚â–‡â–‚â–â–â–‡â–â–â–†â–â–â–†â–â–‚â–‚â–ˆâ–â–â–†â–â–‚â–‡â–‚â–â–†â–‚
wandb:       experiment/TFS_cumulative â–â–â–„â–„â–„â–…â–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 483.35944
wandb:       experiment/TFS_cumulative 299.67433
wandb:  experiment/token_in_cumulative 912823
wandb: experiment/token_out_cumulative 50453
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run fearless-serenity-536 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/2atpnh27
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250127_021617-2atpnh27/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent_universalization
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

Selected template class: Phi4
[2025-01-27 03:10:17,919][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.47s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:06,  2.31s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.33s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:13<00:00,  2.30s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_031033-6rqnhq2l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-darkness-537
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/6rqnhq2l
Storage name: skilled-darkness-537-6rqnhq2l
[2025-01-27 03:10:42,129][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 03:10:45,101][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Phi-4-Univ/skilled-darkness-537/.hydra)... Done. 0.0s
wandb: - 0.018 MB of 0.018 MB uploadedwandb: \ 0.018 MB of 0.018 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.018 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.096 MB uploadedwandb: - 0.097 MB of 0.097 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–‚â–‡â–â–â–‡â–â–â–‡â–‚â–â–‡â–‚â–â–‚â–†â–‚â–â–‡â–‚â–â–‡â–‚â–â–‡â–‚â–‚â–‚â–ˆâ–‚â–‚â–â–ˆâ–â–‚â–†â–‚â–â–‡â–‚
wandb:       experiment/TFS_cumulative â–â–â–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 517.55918
wandb:       experiment/TFS_cumulative 315.4155
wandb:  experiment/token_in_cumulative 985309
wandb: experiment/token_out_cumulative 51400
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run skilled-darkness-537 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/6rqnhq2l
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250127_031033-6rqnhq2l/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent_universalization
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

Selected template class: Phi4
[2025-01-27 04:05:44,451][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.51s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.42s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.36s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.34s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_040600-381mw4zd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-planet-538
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/381mw4zd
Storage name: ruby-planet-538-381mw4zd
[2025-01-27 04:06:08,496][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 04:06:10,244][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Phi-4-Univ/ruby-planet-538/.hydra)... Done. 0.0s
wandb: - 0.018 MB of 0.018 MB uploadedwandb: \ 0.018 MB of 0.018 MB uploadedwandb: | 0.018 MB of 0.018 MB uploadedwandb: / 0.018 MB of 0.018 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.018 MB of 0.018 MB uploadedwandb: \ 0.018 MB of 0.018 MB uploadedwandb: | 0.018 MB of 0.018 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–‚â–‡â–â–â–†â–‚â–â–â–‡â–â–…â–†â–â–‚â–†â–‚â–â–†â–‚â–â–†â–‚â–‚â–‚â–ˆâ–â–‚â–ˆâ–‚â–â–‚â–‡â–‚â–‚â–†â–‚â–â–…â–‚
wandb:       experiment/TFS_cumulative â–â–‚â–„â–…â–„â–„â–…â–…â–…â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 524.93671
wandb:       experiment/TFS_cumulative 320.74585
wandb:  experiment/token_in_cumulative 1012719
wandb: experiment/token_out_cumulative 52011
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run ruby-planet-538 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/381mw4zd
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250127_040600-381mw4zd/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent_universalization
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

Selected template class: Phi4
[2025-01-27 05:01:45,090][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.52s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.42s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.36s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.37s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.35s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_050201-wo5v8vf3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-silence-539
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/wo5v8vf3
Storage name: ethereal-silence-539-wo5v8vf3
[2025-01-27 05:02:09,354][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 05:02:12,683][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Phi-4-Univ/ethereal-silence-539/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                  experiment/TFS â–â–‚â–‡â–â–â–‡â–‚â–â–â–‚â–â–â–‡â–‚â–‚â–†â–‚â–â–‡â–‚â–â–†â–‚â–‚â–â–‡â–â–‚â–â–‚â–â–‚â–ˆâ–‚â–â–†â–‚â–â–†â–‚
wandb:       experiment/TFS_cumulative â–â–ƒâ–„â–…â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–â–ˆâ–â–â–ˆâ–ˆâ–â–ˆâ–ˆâ–â–ˆ
wandb:    persona_0_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_1_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_2_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_3_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    persona_4_collected_resource â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 527.72919
wandb:       experiment/TFS_cumulative 314.78739
wandb:  experiment/token_in_cumulative 1015470
wandb: experiment/token_out_cumulative 53200
wandb:                    num_resource 100
wandb:    persona_0_collected_resource 10
wandb:    persona_1_collected_resource 10
wandb:    persona_2_collected_resource 10
wandb:    persona_3_collected_resource 10
wandb:    persona_4_collected_resource 10
wandb: 
wandb: ðŸš€ View run ethereal-silence-539 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/wo5v8vf3
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250127_050201-wo5v8vf3/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent_universalization
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: true
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: true
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Phi-4-Univ
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--microsoft--phi-4/snapshots/f957856cd926f9d681b14153374d755dd97e45ed
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 150
debug: false

Selected template class: Phi4
[2025-01-27 05:59:04,147][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:02<00:12,  2.51s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 2/6 [00:04<00:09,  2.42s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:07<00:07,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:09<00:04,  2.35s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:11<00:02,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:14<00:00,  2.34s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250127_055920-q5h7kpg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-water-540
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/q5h7kpg1
Storage name: honest-water-540-q5h7kpg1
[2025-01-27 05:59:28,265][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-27 05:59:30,018][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
slurmstepd: error: *** JOB 9616842 ON gcn21 CANCELLED AT 2025-01-27T06:30:44 DUE TO TIME LIMIT ***

JOB STATISTICS
==============
Job ID: 9616842
Cluster: snellius
User/Group: overven/overven
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 13:59:27
CPU Efficiency: 7.77% of 7-12:06:36 core-walltime
Job Wall-clock time: 10:00:22
Memory Utilized: 5.06 GB
Memory Efficiency: 4.21% of 120.00 GB
