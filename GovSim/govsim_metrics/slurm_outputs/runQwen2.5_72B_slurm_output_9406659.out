============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Appending key for api.wandb.ai to your netrc file: /home/overven/.netrc
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: ''
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--Qwen--Qwen1.5-72B-Chat-GPTQ-Int4/snapshots/2c7ffa1f4e88a35aa5d742dfc1e79c3c2df55aa4
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

[2025-01-15 16:44:57,755][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py:1593: UserWarning: Current model requires 23988787584 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|▉         | 1/11 [00:05<00:54,  5.45s/it]Loading checkpoint shards:  18%|█▊        | 2/11 [00:11<00:54,  6.10s/it]Loading checkpoint shards:  27%|██▋       | 3/11 [00:19<00:52,  6.59s/it]Loading checkpoint shards:  36%|███▋      | 4/11 [00:26<00:46,  6.69s/it]Loading checkpoint shards:  45%|████▌     | 5/11 [00:26<00:26,  4.39s/it]Loading checkpoint shards:  55%|█████▍    | 6/11 [00:26<00:14,  3.00s/it]Loading checkpoint shards:  64%|██████▎   | 7/11 [00:26<00:08,  2.12s/it]Loading checkpoint shards:  73%|███████▎  | 8/11 [00:27<00:04,  1.54s/it]Loading checkpoint shards:  82%|████████▏ | 9/11 [00:27<00:02,  1.16s/it]Loading checkpoint shards:  91%|█████████ | 10/11 [00:27<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 11/11 [00:27<00:00,  2.54s/it]
Some weights of the model checkpoint at /gpfs/home2/overven/.cache/huggingface/hub/models--Qwen--Qwen1.5-72B-Chat-GPTQ-Int4/snapshots/2c7ffa1f4e88a35aa5d742dfc1e79c3c2df55aa4 were not used when initializing Qwen2ForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.40.mlp.down_proj.bias', 'model.layers.40.mlp.gate_proj.bias', 'model.layers.40.mlp.up_proj.bias', 'model.layers.40.self_attn.o_proj.bias', 'model.layers.41.mlp.down_proj.bias', 'model.layers.41.mlp.gate_proj.bias', 'model.layers.41.mlp.up_proj.bias', 'model.layers.41.self_attn.o_proj.bias', 'model.layers.42.mlp.down_proj.bias', 'model.layers.42.mlp.gate_proj.bias', 'model.layers.42.mlp.up_proj.bias', 'model.layers.42.self_attn.o_proj.bias', 'model.layers.43.mlp.down_proj.bias', 'model.layers.43.mlp.gate_proj.bias', 'model.layers.43.mlp.up_proj.bias', 'model.layers.43.self_attn.o_proj.bias', 'model.layers.44.mlp.down_proj.bias', 'model.layers.44.mlp.gate_proj.bias', 'model.layers.44.mlp.up_proj.bias', 'model.layers.44.self_attn.o_proj.bias', 'model.layers.45.mlp.down_proj.bias', 'model.layers.45.mlp.gate_proj.bias', 'model.layers.45.mlp.up_proj.bias', 'model.layers.45.self_attn.o_proj.bias', 'model.layers.46.mlp.down_proj.bias', 'model.layers.46.mlp.gate_proj.bias', 'model.layers.46.mlp.up_proj.bias', 'model.layers.46.self_attn.o_proj.bias', 'model.layers.47.mlp.down_proj.bias', 'model.layers.47.mlp.gate_proj.bias', 'model.layers.47.mlp.up_proj.bias', 'model.layers.47.self_attn.o_proj.bias', 'model.layers.48.mlp.down_proj.bias', 'model.layers.48.mlp.gate_proj.bias', 'model.layers.48.mlp.up_proj.bias', 'model.layers.48.self_attn.o_proj.bias', 'model.layers.49.mlp.down_proj.bias', 'model.layers.49.mlp.gate_proj.bias', 'model.layers.49.mlp.up_proj.bias', 'model.layers.49.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.50.mlp.down_proj.bias', 'model.layers.50.mlp.gate_proj.bias', 'model.layers.50.mlp.up_proj.bias', 'model.layers.50.self_attn.o_proj.bias', 'model.layers.51.mlp.down_proj.bias', 'model.layers.51.mlp.gate_proj.bias', 'model.layers.51.mlp.up_proj.bias', 'model.layers.51.self_attn.o_proj.bias', 'model.layers.52.mlp.down_proj.bias', 'model.layers.52.mlp.gate_proj.bias', 'model.layers.52.mlp.up_proj.bias', 'model.layers.52.self_attn.o_proj.bias', 'model.layers.53.mlp.down_proj.bias', 'model.layers.53.mlp.gate_proj.bias', 'model.layers.53.mlp.up_proj.bias', 'model.layers.53.self_attn.o_proj.bias', 'model.layers.54.mlp.down_proj.bias', 'model.layers.54.mlp.gate_proj.bias', 'model.layers.54.mlp.up_proj.bias', 'model.layers.54.self_attn.o_proj.bias', 'model.layers.55.mlp.down_proj.bias', 'model.layers.55.mlp.gate_proj.bias', 'model.layers.55.mlp.up_proj.bias', 'model.layers.55.self_attn.o_proj.bias', 'model.layers.56.mlp.down_proj.bias', 'model.layers.56.mlp.gate_proj.bias', 'model.layers.56.mlp.up_proj.bias', 'model.layers.56.self_attn.o_proj.bias', 'model.layers.57.mlp.down_proj.bias', 'model.layers.57.mlp.gate_proj.bias', 'model.layers.57.mlp.up_proj.bias', 'model.layers.57.self_attn.o_proj.bias', 'model.layers.58.mlp.down_proj.bias', 'model.layers.58.mlp.gate_proj.bias', 'model.layers.58.mlp.up_proj.bias', 'model.layers.58.self_attn.o_proj.bias', 'model.layers.59.mlp.down_proj.bias', 'model.layers.59.mlp.gate_proj.bias', 'model.layers.59.mlp.up_proj.bias', 'model.layers.59.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.60.mlp.down_proj.bias', 'model.layers.60.mlp.gate_proj.bias', 'model.layers.60.mlp.up_proj.bias', 'model.layers.60.self_attn.o_proj.bias', 'model.layers.61.mlp.down_proj.bias', 'model.layers.61.mlp.gate_proj.bias', 'model.layers.61.mlp.up_proj.bias', 'model.layers.61.self_attn.o_proj.bias', 'model.layers.62.mlp.down_proj.bias', 'model.layers.62.mlp.gate_proj.bias', 'model.layers.62.mlp.up_proj.bias', 'model.layers.62.self_attn.o_proj.bias', 'model.layers.63.mlp.down_proj.bias', 'model.layers.63.mlp.gate_proj.bias', 'model.layers.63.mlp.up_proj.bias', 'model.layers.63.self_attn.o_proj.bias', 'model.layers.64.mlp.down_proj.bias', 'model.layers.64.mlp.gate_proj.bias', 'model.layers.64.mlp.up_proj.bias', 'model.layers.64.self_attn.o_proj.bias', 'model.layers.65.mlp.down_proj.bias', 'model.layers.65.mlp.gate_proj.bias', 'model.layers.65.mlp.up_proj.bias', 'model.layers.65.self_attn.o_proj.bias', 'model.layers.66.mlp.down_proj.bias', 'model.layers.66.mlp.gate_proj.bias', 'model.layers.66.mlp.up_proj.bias', 'model.layers.66.self_attn.o_proj.bias', 'model.layers.67.mlp.down_proj.bias', 'model.layers.67.mlp.gate_proj.bias', 'model.layers.67.mlp.up_proj.bias', 'model.layers.67.self_attn.o_proj.bias', 'model.layers.68.mlp.down_proj.bias', 'model.layers.68.mlp.gate_proj.bias', 'model.layers.68.mlp.up_proj.bias', 'model.layers.68.self_attn.o_proj.bias', 'model.layers.69.mlp.down_proj.bias', 'model.layers.69.mlp.gate_proj.bias', 'model.layers.69.mlp.up_proj.bias', 'model.layers.69.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.70.mlp.down_proj.bias', 'model.layers.70.mlp.gate_proj.bias', 'model.layers.70.mlp.up_proj.bias', 'model.layers.70.self_attn.o_proj.bias', 'model.layers.71.mlp.down_proj.bias', 'model.layers.71.mlp.gate_proj.bias', 'model.layers.71.mlp.up_proj.bias', 'model.layers.71.self_attn.o_proj.bias', 'model.layers.72.mlp.down_proj.bias', 'model.layers.72.mlp.gate_proj.bias', 'model.layers.72.mlp.up_proj.bias', 'model.layers.72.self_attn.o_proj.bias', 'model.layers.73.mlp.down_proj.bias', 'model.layers.73.mlp.gate_proj.bias', 'model.layers.73.mlp.up_proj.bias', 'model.layers.73.self_attn.o_proj.bias', 'model.layers.74.mlp.down_proj.bias', 'model.layers.74.mlp.gate_proj.bias', 'model.layers.74.mlp.up_proj.bias', 'model.layers.74.self_attn.o_proj.bias', 'model.layers.75.mlp.down_proj.bias', 'model.layers.75.mlp.gate_proj.bias', 'model.layers.75.mlp.up_proj.bias', 'model.layers.75.self_attn.o_proj.bias', 'model.layers.76.mlp.down_proj.bias', 'model.layers.76.mlp.gate_proj.bias', 'model.layers.76.mlp.up_proj.bias', 'model.layers.76.self_attn.o_proj.bias', 'model.layers.77.mlp.down_proj.bias', 'model.layers.77.mlp.gate_proj.bias', 'model.layers.77.mlp.up_proj.bias', 'model.layers.77.self_attn.o_proj.bias', 'model.layers.78.mlp.down_proj.bias', 'model.layers.78.mlp.gate_proj.bias', 'model.layers.78.mlp.up_proj.bias', 'model.layers.78.self_attn.o_proj.bias', 'model.layers.79.mlp.down_proj.bias', 'model.layers.79.mlp.gate_proj.bias', 'model.layers.79.mlp.up_proj.bias', 'model.layers.79.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias']
- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Error executing job with overrides: ['experiment=fish_baseline_concurrent', 'llm.path=/gpfs/home2/overven/.cache/huggingface/hub/models--Qwen--Qwen1.5-72B-Chat-GPTQ-Int4/snapshots/2c7ffa1f4e88a35aa5d742dfc1e79c3c2df55aa4']
Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/main.py", line 27, in main
    model = get_model(cfg.llm.path, cfg.llm.is_api, cfg.seed, cfg.llm.backend)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/loader.py", line 110, in get_model
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3820, in from_pretrained
    dispatch_model(model, **device_map_kwargs)
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/big_modeling.py", line 424, in dispatch_model
    attach_align_device_hook_on_blocks(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 664, in attach_align_device_hook_on_blocks
    attach_align_device_hook_on_blocks(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 664, in attach_align_device_hook_on_blocks
    attach_align_device_hook_on_blocks(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 664, in attach_align_device_hook_on_blocks
    attach_align_device_hook_on_blocks(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 627, in attach_align_device_hook_on_blocks
    attach_align_device_hook(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 518, in attach_align_device_hook
    attach_align_device_hook(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 518, in attach_align_device_hook
    attach_align_device_hook(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 509, in attach_align_device_hook
    add_hook_to_module(module, hook, append=True)
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 161, in add_hook_to_module
    module = hook.init_hook(module)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 312, in init_hook
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 321, in set_module_tensor_to_device
    new_value = old_value.to(device)
                ^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 91.88 MiB is free. Process 1084413 has 826.00 MiB memory in use. Including non-PyTorch memory, this process has 19.49 GiB memory in use. Of the allocated memory 19.24 GiB is allocated by PyTorch, and 85.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.

JOB STATISTICS
==============
Job ID: 9406659
Cluster: snellius
User/Group: overven/overven
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 9
CPU Utilized: 00:01:01
CPU Efficiency: 9.82% of 00:10:21 core-walltime
Job Wall-clock time: 00:01:09
Memory Utilized: 5.20 GB
Memory Efficiency: 8.66% of 60.00 GB
