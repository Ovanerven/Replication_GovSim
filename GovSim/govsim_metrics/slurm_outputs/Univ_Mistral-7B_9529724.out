============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Appending key for api.wandb.ai to your netrc file: /home/overven/.netrc
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

[2025-01-22 06:45:31,907][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:12,  6.06s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.01s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.84s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_064551-9015qfvj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-elevator-202
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/9015qfvj
Storage name: sage-elevator-202-9015qfvj
[2025-01-22 06:46:01,754][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 06:46:04,674][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.

Returning default value in find
  warnings.warn(
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Univ_Mistral-7B-Instruct-v0.2/sage-elevator-202/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–
wandb:                  experiment/TFS â–â–â–â–â–â–„â–‚â–„â–…â–„â–„â–ƒâ–ˆâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–„
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 20
wandb:                  experiment/TFS 800.98236
wandb:       experiment/TFS_cumulative 433.74323
wandb:  experiment/token_in_cumulative 61336
wandb: experiment/token_out_cumulative 3578
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 30
wandb:    persona_2_collected_resource 0
wandb:    persona_3_collected_resource 150
wandb:    persona_4_collected_resource 70
wandb: 
wandb: ðŸš€ View run sage-elevator-202 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/9015qfvj
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_064551-9015qfvj/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

[2025-01-22 06:48:46,444][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.34s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.26s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_064855-pyoufmni
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-sponge-203
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/pyoufmni
Storage name: distinctive-sponge-203-pyoufmni
[2025-01-22 06:49:03,283][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 06:49:04,881][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.

Returning default value in find
  warnings.warn(
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Univ_Mistral-7B-Instruct-v0.2/distinctive-sponge-203/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–
wandb:                  experiment/TFS â–â–â–â–â–â–†â–„â–…â–†â–ˆâ–†â–‡â–ƒâ–…â–…â–…â–†â–†â–…â–‡â–…â–ˆâ–‡â–‡
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 16
wandb:                  experiment/TFS 824.95429
wandb:       experiment/TFS_cumulative 413.62677
wandb:  experiment/token_in_cumulative 61363
wandb: experiment/token_out_cumulative 3822
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 30
wandb:    persona_2_collected_resource 0
wandb:    persona_3_collected_resource 150
wandb:    persona_4_collected_resource 70
wandb: 
wandb: ðŸš€ View run distinctive-sponge-203 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/pyoufmni
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_064855-pyoufmni/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

[2025-01-22 06:51:55,284][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.34s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_065203-7w8wt2vq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-water-205
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/7w8wt2vq
Storage name: winter-water-205-7w8wt2vq
[2025-01-22 06:52:12,243][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 06:52:13,774][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.

Returning default value in find
  warnings.warn(
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A. The conversation focused on the impact of catch sizes on the lake's ecosystem and income, but there was no explicit agreement on a numerical catch limit for each person.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A. The conversation focused on the impact of catch sizes on the lake's ecosystem and income, but there was no explicit agreement on a numerical catch limit for each person.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Univ_Mistral-7B-Instruct-v0.2/winter-water-205/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–„â–ƒâ–ƒâ–ƒâ–„â–…â–…â–†â–„â–ˆâ–…â–…â–…â–†â–…â–ƒâ–…â–ƒâ–„â–†
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–‚â–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 980.86453
wandb:       experiment/TFS_cumulative 441.78991
wandb:  experiment/token_in_cumulative 70906
wandb: experiment/token_out_cumulative 4085
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 30
wandb:    persona_2_collected_resource 0
wandb:    persona_3_collected_resource 150
wandb:    persona_4_collected_resource 70
wandb: 
wandb: ðŸš€ View run winter-water-205 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/7w8wt2vq
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_065203-7w8wt2vq/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 150
debug: false

[2025-01-22 06:55:16,517][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.26s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_065525-pleoyqlm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-music-207
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/pleoyqlm
Storage name: glorious-music-207-pleoyqlm
[2025-01-22 06:55:34,331][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 06:55:35,867][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.

Returning default value in find
  warnings.warn(
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in A rotating catch limit system was proposed, but no specific numerical catch limit per person was agreed upon in the conversation. The system would involve each person taking turns catching a larger share of the fish, allowing the population to recover and reproduce before the next round of fishing.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in A rotating catch limit system was proposed, but no specific numerical catch limit per person was agreed upon in the conversation. The system would involve each person taking turns catching a larger share of the fish, allowing the population to recover and reproduce before the next round of fishing.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Univ_Mistral-7B-Instruct-v0.2/glorious-music-207/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–†â–ƒâ–†â–ˆâ–†â–ƒâ–„â–…â–†â–†â–…â–†â–‡â–‡â–„â–‡â–ˆ
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 975.59707
wandb:       experiment/TFS_cumulative 369.55355
wandb:  experiment/token_in_cumulative 44592
wandb: experiment/token_out_cumulative 3220
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 30
wandb:    persona_2_collected_resource 0
wandb:    persona_3_collected_resource 150
wandb:    persona_4_collected_resource 70
wandb: 
wandb: ðŸš€ View run glorious-music-207 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/pleoyqlm
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_065525-pleoyqlm/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 200
debug: false

[2025-01-22 06:57:57,798][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.26s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_065806-vv8ltxvt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-valley-208
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/vv8ltxvt
Storage name: dainty-valley-208-vv8ltxvt
[2025-01-22 06:58:14,759][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 06:58:16,384][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in Jack would catch a moderate amount of fish, but the exact number would depend on his negotiations and cooperation with the other fishermen.

Returning default value in find
  warnings.warn(
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in The conversation discussed implementing a rotating catch limit system, where each month one person catches a larger share of the fish, and the others catch fewer tons. However, no specific numerical catch limit per person was agreed upon in the conversation.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in The conversation discussed implementing a rotating catch limit system, where each month one person catches a larger share of the fish, and the others catch fewer tons. However, no specific numerical catch limit per person was agreed upon in the conversation.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Univ_Mistral-7B-Instruct-v0.2/dainty-valley-208/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–†â–ƒâ–‡â–ˆâ–…â–ƒâ–…â–…â–†â–…â–…â–…â–†â–‡â–†â–ƒâ–„
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 509.61472
wandb:       experiment/TFS_cumulative 348.94401
wandb:  experiment/token_in_cumulative 45484
wandb: experiment/token_out_cumulative 3474
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 30
wandb:    persona_2_collected_resource 0
wandb:    persona_3_collected_resource 150
wandb:    persona_4_collected_resource 70
wandb: 
wandb: ðŸš€ View run dainty-valley-208 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/vv8ltxvt
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_065806-vv8ltxvt/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

[2025-01-22 07:00:50,594][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:08,  4.38s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:06<00:03,  3.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.66s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.91s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_070101-3x3io6um
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-dragon-209
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/3x3io6um
Storage name: dazzling-dragon-209-3x3io6um
[2025-01-22 07:01:09,360][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:01:10,894][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Univ_Mistral-7B-Instruct-v0.2/dazzling-dragon-209/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–‚â–â–â–â–†â–ƒâ–„â–…â–ƒâ–„â–ˆâ–ˆâ–†â–„â–†â–†â–†â–†â–†â–†â–…â–…â–…â–†â–…
wandb:       experiment/TFS_cumulative â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 734.11322
wandb:       experiment/TFS_cumulative 520.12401
wandb:  experiment/token_in_cumulative 78010
wandb: experiment/token_out_cumulative 3543
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 120
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 70
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run dazzling-dragon-209 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/3x3io6um
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_070101-3x3io6um/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/threading.py", line 1038, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/threading.py", line 1038, in _bootstrap_inner
    self.run()
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/threading.py", line 975, in run
    self.run()
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/threading.py", line 975, in run
    self._target(*self._args, **self._kwargs)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
    self._loop_check_status(
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
    local_handle = request()
                   ^^^^^^^^^
                   ^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface.py", line 756, in deliver_network_status
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface.py", line 764, in deliver_internal_messages
    return self._deliver_network_status(status)
    return self._deliver_internal_messages(internal_message)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py", line 484, in _deliver_network_status
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py", line 490, in _deliver_internal_messages
    return self._deliver_record(record)
    return self._deliver_record(record)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py", line 437, in _deliver_record
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py", line 437, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
    handle = mailbox._deliver_record(record, interface=self)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
    interface._publish(record)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
      File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
self._send_message(msg)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home/overven/.local/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
           ^^^^^^^^^^^^^^^^^^^^^
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
           ^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

[2025-01-22 07:04:01,535][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.33s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_070410-b0vblk5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-leaf-210
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/b0vblk5e
Storage name: polished-leaf-210-b0vblk5e
[2025-01-22 07:04:18,503][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:04:19,862][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Univ_Mistral-7B-Instruct-v0.2/polished-leaf-210/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–
wandb:                  experiment/TFS â–â–‚â–â–â–â–ƒâ–†â–…â–‚â–‚â–…â–„â–…â–„â–„â–‡â–ˆâ–†â–‡â–†
wandb:       experiment/TFS_cumulative â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–‡â–‡â–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 8
wandb:                  experiment/TFS 816.76039
wandb:       experiment/TFS_cumulative 440.58492
wandb:  experiment/token_in_cumulative 53644
wandb: experiment/token_out_cumulative 2977
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 120
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 70
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run polished-leaf-210 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/b0vblk5e
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_070410-b0vblk5e/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

[2025-01-22 07:06:43,345][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.32s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.24s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_070651-x0r7b5ul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-snowflake-211
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/x0r7b5ul
Storage name: exalted-snowflake-211-x0r7b5ul
[2025-01-22 07:07:00,189][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:07:01,598][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Univ_Mistral-7B-Instruct-v0.2/exalted-snowflake-211/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–ƒâ–â–â–â–…â–†â–â–‚â–„â–…â–…â–…â–…â–ˆâ–‡â–‡â–†â–ˆ
wandb:       experiment/TFS_cumulative â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–‡â–‡â–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 922.18074
wandb:       experiment/TFS_cumulative 376.69912
wandb:  experiment/token_in_cumulative 39909
wandb: experiment/token_out_cumulative 2857
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 120
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 70
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run exalted-snowflake-211 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/x0r7b5ul
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_070651-x0r7b5ul/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 150
debug: false

[2025-01-22 07:09:08,259][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.36s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.37s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.29s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_070916-rh6nym1f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-hill-212
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/rh6nym1f
Storage name: northern-hill-212-rh6nym1f
[2025-01-22 07:09:25,191][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:09:26,660][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A. The conversation focused on strategies for maximizing income and ensuring long-term sustainability, but there was no explicit agreement on a concrete grass usage limit per person.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A. The conversation focused on strategies for maximizing income and ensuring long-term sustainability, but there was no explicit agreement on a concrete grass usage limit per person.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Univ_Mistral-7B-Instruct-v0.2/northern-hill-212/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–‚â–â–â–â–†â–„â–…â–…â–…â–†â–†â–†â–ƒâ–ˆâ–…â–…â–…â–…â–…â–…â–‡â–…â–…â–…
wandb:       experiment/TFS_cumulative â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 682.4382
wandb:       experiment/TFS_cumulative 494.06291
wandb:  experiment/token_in_cumulative 73821
wandb: experiment/token_out_cumulative 3405
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 120
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 70
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run northern-hill-212 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/rh6nym1f
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_070916-rh6nym1f/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: sheep_${code_version}/${group_name}
  scenario: sheep
  env:
    name: sheep_baseline_concurrent
    class_name: sheep_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 200
debug: false

[2025-01-22 07:12:16,981][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.29s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.18s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.21s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_071225-ahabzw8g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-bee-213
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/ahabzw8g
Storage name: polar-bee-213-ahabzw8g
[2025-01-22 07:12:36,630][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:12:38,273][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A. The conversation focused on strategies for optimizing grass consumption and income, as well as the importance of communication and collaboration. While the participants discussed the need to coordinate flock sizes based on current grass levels to avoid overgrazing, no explicit agreement on a concrete usage limit per person was reached during the conversation.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A. The conversation focused on strategies for optimizing grass consumption and income, as well as the importance of communication and collaboration. While the participants discussed the need to coordinate flock sizes based on current grass levels to avoid overgrazing, no explicit agreement on a concrete usage limit per person was reached during the conversation.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/sheep_v6.4/Univ_Mistral-7B-Instruct-v0.2/polar-bee-213/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–ƒâ–â–â–â–…â–„â–†â–†â–†â–…â–ˆâ–ˆâ–‡â–„â–†â–‡â–‡â–†â–‡â–†â–‡â–†â–‡â–†â–‡
wandb:       experiment/TFS_cumulative â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 888.85972
wandb:       experiment/TFS_cumulative 512.16415
wandb:  experiment/token_in_cumulative 76172
wandb: experiment/token_out_cumulative 3375
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 120
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 70
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run polar-bee-213 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/ahabzw8g
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_071225-ahabzw8g/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

[2025-01-22 07:15:26,418][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.30s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.19s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_071534-26iuxzt3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-forest-214
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/26iuxzt3
Storage name: wandering-forest-214-26iuxzt3
[2025-01-22 07:15:43,195][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:15:44,772][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in The conversation resulted in proposed production levels for each participant, but there was no explicit agreement on a concrete production limit that all participants agreed to keep. Instead, each participant proposed their own production level, and they were open to discussing adjustments or compromises to reach a consensus. Therefore, no specific production limit was agreed upon in the conversation.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in The conversation resulted in proposed production levels for each participant, but there was no explicit agreement on a concrete production limit that all participants agreed to keep. Instead, each participant proposed their own production level, and they were open to discussing adjustments or compromises to reach a consensus. Therefore, no specific production limit was agreed upon in the conversation.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Univ_Mistral-7B-Instruct-v0.2/wandering-forest-214/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.017 MB of 0.017 MB uploadedwandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–…â–…â–…â–„â–…â–…â–…â–…â–„â–…â–…â–†â–…â–…â–†â–ˆâ–†â–†â–†â–„
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–‚â–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 541.7446
wandb:       experiment/TFS_cumulative 471.6634
wandb:  experiment/token_in_cumulative 68217
wandb: experiment/token_out_cumulative 3345
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 25
wandb:    persona_1_collected_resource 45
wandb:    persona_2_collected_resource 40
wandb:    persona_3_collected_resource 25
wandb:    persona_4_collected_resource 1
wandb: 
wandb: ðŸš€ View run wandering-forest-214 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/26iuxzt3
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_071534-26iuxzt3/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

[2025-01-22 07:18:29,312][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.33s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_071837-04f8nyil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-yogurt-215
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/04f8nyil
Storage name: comfy-yogurt-215-04f8nyil
[2025-01-22 07:18:46,303][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:18:47,970][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Univ_Mistral-7B-Instruct-v0.2/comfy-yogurt-215/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–
wandb:                  experiment/TFS â–â–â–â–â–â–†â–…â–†â–‡â–†â–†â–‡â–‡â–ˆâ–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ƒâ–ˆâ–‡â–‡
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 10
wandb:                  experiment/TFS 813.515
wandb:       experiment/TFS_cumulative 479.5069
wandb:  experiment/token_in_cumulative 75250
wandb: experiment/token_out_cumulative 3663
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 25
wandb:    persona_1_collected_resource 45
wandb:    persona_2_collected_resource 40
wandb:    persona_3_collected_resource 25
wandb:    persona_4_collected_resource 1
wandb: 
wandb: ðŸš€ View run comfy-yogurt-215 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/04f8nyil
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_071837-04f8nyil/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

[2025-01-22 07:21:46,371][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.35s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.26s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_072154-c2wr5ifh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-violet-216
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/c2wr5ifh
Storage name: clear-violet-216-c2wr5ifh
[2025-01-22 07:22:03,305][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:22:04,926][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Univ_Mistral-7B-Instruct-v0.2/clear-violet-216/.hydra)... Done. 0.0s
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–†â–†â–†â–ˆâ–…â–‡â–ˆâ–‡â–‡â–„â–…â–†â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–‡
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 774.41801
wandb:       experiment/TFS_cumulative 456.5737
wandb:  experiment/token_in_cumulative 69743
wandb: experiment/token_out_cumulative 3726
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 25
wandb:    persona_1_collected_resource 45
wandb:    persona_2_collected_resource 40
wandb:    persona_3_collected_resource 25
wandb:    persona_4_collected_resource 1
wandb: 
wandb: ðŸš€ View run clear-violet-216 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/c2wr5ifh
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_072154-c2wr5ifh/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 150
debug: false

[2025-01-22 07:24:58,957][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.33s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.32s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.22s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_072507-fecj9t9c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-universe-217
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/fecj9t9c
Storage name: likely-universe-217-fecj9t9c
[2025-01-22 07:25:15,781][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:25:17,322][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in Based on the conversation, each participant proposed a production level for the upcoming month, but there was no explicit agreement on a concrete production limit for any individual or the group as a whole. Instead, they agreed to work together to find a balanced solution that minimizes their collective impact on the river while still achieving their financial objectives. Therefore, N/A.: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in Based on the conversation, each participant proposed a production level for the upcoming month, but there was no explicit agreement on a concrete production limit for any individual or the group as a whole. Instead, they agreed to work together to find a balanced solution that minimizes their collective impact on the river while still achieving their financial objectives. Therefore, N/A.

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Univ_Mistral-7B-Instruct-v0.2/likely-universe-217/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–…â–†â–†â–…â–…â–…â–…â–†â–‡â–ƒâ–…â–†â–…â–†â–†â–†â–…â–ˆâ–‡â–ˆâ–‡
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 945.61664
wandb:       experiment/TFS_cumulative 479.87189
wandb:  experiment/token_in_cumulative 67203
wandb: experiment/token_out_cumulative 3243
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 25
wandb:    persona_1_collected_resource 45
wandb:    persona_2_collected_resource 40
wandb:    persona_3_collected_resource 25
wandb:    persona_4_collected_resource 1
wandb: 
wandb: ðŸš€ View run likely-universe-217 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/fecj9t9c
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_072507-fecj9t9c/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
  universalization: true
code_version: v6.4
group_name: Univ_Mistral-7B-Instruct-v0.2
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/3ad372fc79158a2148299e3318516c786aeded6c
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 200
debug: false

[2025-01-22 07:27:57,917][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:04,  2.32s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:04<00:02,  2.31s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.23s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250122_072806-1wbzfllq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-pond-218
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/1wbzfllq
Storage name: fancy-pond-218-1wbzfllq
[2025-01-22 07:28:14,662][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
[2025-01-22 07:28:16,304][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/pollution_v6.4/Univ_Mistral-7B-Instruct-v0.2/fancy-pond-218/.hydra)... Done. 0.0s
wandb: - 0.017 MB of 0.017 MB uploadedwandb: \ 0.017 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.017 MB of 0.017 MB uploadedwandb: - 0.017 MB of 0.017 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–
wandb:                  experiment/TFS â–â–â–â–â–â–†â–ˆâ–„â–‚â–ƒâ–…â–…â–…â–…â–…â–‡â–ˆâ–ˆâ–ˆâ–‡
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–†â–†â–‡â–‡â–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 50
wandb:                  experiment/TFS 705.26346
wandb:       experiment/TFS_cumulative 373.68407
wandb:  experiment/token_in_cumulative 46750
wandb: experiment/token_out_cumulative 3106
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 25
wandb:    persona_1_collected_resource 45
wandb:    persona_2_collected_resource 40
wandb:    persona_3_collected_resource 25
wandb:    persona_4_collected_resource 1
wandb: 
wandb: ðŸš€ View run fancy-pond-218 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/1wbzfllq
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250122_072806-1wbzfllq/logs

JOB STATISTICS
==============
Job ID: 9529724
Cluster: snellius
User/Group: overven/overven
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 9
CPU Utilized: 01:16:24
CPU Efficiency: 18.59% of 06:51:00 core-walltime
Job Wall-clock time: 00:45:40
Memory Utilized: 4.55 GB
Memory Efficiency: 7.58% of 60.00 GB
