============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Appending key for api.wandb.ai to your netrc file: /home/overven/.netrc
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Meta-Llama-2-13B
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/llama13B/models--meta-llama--Llama-2-13b-chat-hf/snapshots/a2cb7a712bb6e5e736ca7f8cd98167f81a0b5bd8
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

[2025-01-20 11:15:58,085][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:11<00:23, 11.56s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:20<00:10, 10.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  5.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:20<00:00,  6.97s/it]
[2025-01-20 11:16:19,138][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250120_111623-ibm3aa6k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-dawn-75
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/ibm3aa6k
Storage name: olive-dawn-75-ibm3aa6k
[2025-01-20 11:16:32,955][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
/home/overven/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-01-20 11:16:35,253][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 283.88 MiB is free. Including non-PyTorch memory, this process has 19.30 GiB memory in use. Of the allocated memory 18.78 GiB is allocated by PyTorch, and 315.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 283.88 MiB is free. Including non-PyTorch memory, this process has 19.30 GiB memory in use. Of the allocated memory 18.78 GiB is allocated by PyTorch, and 315.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 283.88 MiB is free. Including non-PyTorch memory, this process has 19.30 GiB memory in use. Of the allocated memory 18.93 GiB is allocated by PyTorch, and 162.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 283.88 MiB is free. Including non-PyTorch memory, this process has 19.30 GiB memory in use. Of the allocated memory 18.93 GiB is allocated by PyTorch, and 162.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Of the allocated memory 18.75 GiB is allocated by PyTorch, and 319.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Of the allocated memory 18.75 GiB is allocated by PyTorch, and 319.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: gen(num_resource, 100, 0.0, 1.0, 1.0, \d+, None) can be used only in assistant block, not in user block!: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 91, in __add__
    raise Exception(
Exception: gen(num_resource, 100, 0.0, 1.0, 1.0, \d+, None) can be used only in assistant block, not in user block!

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Meta-Llama-2-13B/olive-dawn-75/.hydra)... Done. 0.0s
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–â–â–â–‚â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–„â–…â–…â–†â–†â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 15.25376
wandb:       experiment/TFS_cumulative 11.14967
wandb:  experiment/token_in_cumulative 60523
wandb: experiment/token_out_cumulative 5137
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 101
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run olive-dawn-75 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/ibm3aa6k
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250120_111623-ibm3aa6k/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Meta-Llama-2-13B
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/llama13B/models--meta-llama--Llama-2-13b-chat-hf/snapshots/a2cb7a712bb6e5e736ca7f8cd98167f81a0b5bd8
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

[2025-01-20 12:55:02,551][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:11<00:23, 11.58s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:15<00:06,  6.88s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  3.79s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.10s/it]
[2025-01-20 12:55:17,984][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250120_125522-9dr9383a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-dew-76
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/9dr9383a
Storage name: stellar-dew-76-9dr9383a
[2025-01-20 12:55:31,801][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
/home/overven/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-01-20 12:55:34,450][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 183.88 MiB is free. Including non-PyTorch memory, this process has 19.40 GiB memory in use. Of the allocated memory 19.01 GiB is allocated by PyTorch, and 178.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 183.88 MiB is free. Including non-PyTorch memory, this process has 19.40 GiB memory in use. Of the allocated memory 19.01 GiB is allocated by PyTorch, and 178.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 243.88 MiB is free. Including non-PyTorch memory, this process has 19.34 GiB memory in use. Of the allocated memory 19.02 GiB is allocated by PyTorch, and 109.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 243.88 MiB is free. Including non-PyTorch memory, this process has 19.34 GiB memory in use. Of the allocated memory 19.02 GiB is allocated by PyTorch, and 109.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 243.88 MiB is free. Including non-PyTorch memory, this process has 19.34 GiB memory in use. Of the allocated memory 19.03 GiB is allocated by PyTorch, and 99.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 243.88 MiB is free. Including non-PyTorch memory, this process has 19.34 GiB memory in use. Of the allocated memory 19.03 GiB is allocated by PyTorch, and 99.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in 

N/A (No explicit agreement on a concrete fishing limit was made during the conversation.): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in 

N/A (No explicit agreement on a concrete fishing limit was made during the conversation.)

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Meta-Llama-2-13B/stellar-dew-76/.hydra)... Done. 0.0s
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–â–â–â–â–â–‚â–ˆâ–ˆâ–â–â–â–â–â–â–â–‚â–â–â–â–
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 23.62473
wandb:       experiment/TFS_cumulative 13.76554
wandb:  experiment/token_in_cumulative 65150
wandb: experiment/token_out_cumulative 5011
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 101
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run stellar-dew-76 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/9dr9383a
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250120_125522-9dr9383a/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Meta-Llama-2-13B
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/llama13B/models--meta-llama--Llama-2-13b-chat-hf/snapshots/a2cb7a712bb6e5e736ca7f8cd98167f81a0b5bd8
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

[2025-01-20 14:20:50,024][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:12<00:24, 12.40s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:15<00:07,  7.14s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  3.93s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.33s/it]
[2025-01-20 14:21:06,147][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250120_142110-8rz92cad
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run quiet-plant-77
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/8rz92cad
Storage name: quiet-plant-77-8rz92cad
[2025-01-20 14:21:19,953][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
/home/overven/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-01-20 14:21:23,047][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in N/A (No explicit agreement on a concrete fishing limit was made during the conversation.): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in N/A (No explicit agreement on a concrete fishing limit was made during the conversation.)

Returning default value in find
  warnings.warn(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Meta-Llama-2-13B/quiet-plant-77/.hydra)... Done. 0.0s
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: 
wandb: Run history:
wandb:                  experiment/TFS â–â–â–â–â–â–…â–‡â–†â–…â–†â–†â–‡â–‡â–ƒâ–…â–†â–‡â–‡â–†â–†â–ƒâ–ƒâ–ƒâ–ˆâ–„
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb: experiment/token_out_cumulative â–â–‚â–ƒâ–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–
wandb:    persona_1_collected_resource â–
wandb:    persona_2_collected_resource â–
wandb:    persona_3_collected_resource â–
wandb:    persona_4_collected_resource â–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 22.24352
wandb:       experiment/TFS_cumulative 13.1689
wandb:  experiment/token_in_cumulative 58830
wandb: experiment/token_out_cumulative 4811
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 40
wandb:    persona_1_collected_resource 50
wandb:    persona_2_collected_resource 50
wandb:    persona_3_collected_resource 101
wandb:    persona_4_collected_resource 50
wandb: 
wandb: ðŸš€ View run quiet-plant-77 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/8rz92cad
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250120_142110-8rz92cad/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: pollution_${code_version}/${group_name}
  scenario: pollution
  env:
    name: pollution_baseline_concurrent
    class_name: pollution_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Meta-Llama-2-13B
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/llama13B/models--meta-llama--Llama-2-13b-chat-hf/snapshots/a2cb7a712bb6e5e736ca7f8cd98167f81a0b5bd8
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 1
debug: false

[2025-01-20 15:42:14,998][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:11<00:22, 11.36s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:14<00:06,  6.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.97s/it]
[2025-01-20 15:42:30,046][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250120_154234-0cl7ru1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-galaxy-78
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/0cl7ru1q
Storage name: proud-galaxy-78-0cl7ru1q
[2025-01-20 15:42:43,682][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
/home/overven/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-01-20 15:42:46,717][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.84 GiB is allocated by PyTorch, and 228.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.84 GiB is allocated by PyTorch, and 228.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.82 GiB is allocated by PyTorch, and 250.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.82 GiB is allocated by PyTorch, and 250.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.82 GiB is allocated by PyTorch, and 247.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.82 GiB is allocated by PyTorch, and 247.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:111: RuntimeWarning: An exception occured: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.78 GiB is allocated by PyTorch, and 286.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 101, in gen
    lm: Model = previous_lm + pathfinder.gen(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 96, in __add__
    res = lm._get_gen(value)
          ^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 157, in _get_gen
    output = self.model.generate(
             ^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 1736, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/generation/utils.py", line 2375, in _sample
    outputs = self(
              ^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    logits = self.lm_head(hidden_states)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.conda/envs/GovComGPTQ/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/hooks.py", line 355, in pre_forward
    set_module_tensor_to_device(
  File "/home/overven/.local/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 329, in set_module_tensor_to_device
    new_value = value.to(device)
                ^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 19.62 GiB of which 303.88 MiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Process 3791155 has 6.24 GiB memory in use. Of the allocated memory 18.78 GiB is allocated by PyTorch, and 286.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Returning default value in gen
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in 

N/A (No explicit agreement on a concrete widget production limit was made during the conversation.): Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in 

N/A (No explicit agreement on a concrete widget production limit was made during the conversation.)

Returning default value in find
  warnings.warn(
slurmstepd: error: *** JOB 9503625 ON gcn2 CANCELLED AT 2025-01-20T16:59:47 ***

JOB STATISTICS
==============
Job ID: 9503625
Cluster: snellius
User/Group: overven/overven
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 9
CPU Utilized: 04:31:55
CPU Efficiency: 8.77% of 2-03:38:51 core-walltime
Job Wall-clock time: 05:44:19
Memory Utilized: 17.40 GB
Memory Efficiency: 28.99% of 60.00 GB
