============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
wandb: Appending key for api.wandb.ai to your netrc file: /home/overven/.netrc
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Meta-Llama-3-8B
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 42
debug: false

[2025-01-19 14:17:59,052][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:06,  2.28s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:04<00:04,  2.29s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:06<00:02,  2.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.85s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250119_141808-zpa8dk0n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-night-45
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/zpa8dk0n
Storage name: resilient-night-45-zpa8dk0n
[2025-01-19 14:18:16,972][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
/home/overven/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-01-19 14:18:18,653][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Meta-Llama-3-8B/resilient-night-45/.hydra)... Done. 0.0s
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–ˆâ–
wandb:                  experiment/TFS â–â–â–â–â–„â–„â–„â–†â–…â–†â–†â–„â–…â–‡â–‡â–‡â–†â–†â–†â–…â–‚â–„â–‚â–â–†â–…â–†â–†â–†â–…â–‡â–„â–ƒâ–‡â–‡â–†â–‡â–ˆâ–ˆâ–ˆ
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–ˆâ–
wandb:    persona_1_collected_resource â–ˆâ–
wandb:    persona_2_collected_resource â–ˆâ–
wandb:    persona_3_collected_resource â–ˆâ–
wandb:    persona_4_collected_resource â–ˆâ–
wandb: 
wandb: Run summary:
wandb:     conversation_resource_limit 8
wandb:                  experiment/TFS 927.82214
wandb:       experiment/TFS_cumulative 533.64508
wandb:  experiment/token_in_cumulative 191100
wandb: experiment/token_out_cumulative 8501
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 11
wandb:    persona_1_collected_resource 5
wandb:    persona_2_collected_resource 6
wandb:    persona_3_collected_resource 8
wandb:    persona_4_collected_resource 8
wandb: 
wandb: ðŸš€ View run resilient-night-45 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/zpa8dk0n
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250119_141808-zpa8dk0n/logs
/gpfs/home2/overven/GovSim/simulation/main.py:87: UserWarning: register_resolver() is deprecated.
See https://github.com/omry/omegaconf/issues/426 for migration instructions.

  OmegaConf.register_resolver("uuid", lambda: f"run_{uuid.uuid4()}")
/home/overven/.local/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
experiment:
  personas:
    persona_0:
      name: John
      goals: ''
    persona_1:
      name: Kate
      goals: ''
    persona_2:
      name: Jack
      goals: ''
    persona_3:
      name: Emma
      goals: ''
    persona_4:
      name: Luke
      goals: ''
    num: 5
  name: fishing_${code_version}/${group_name}
  scenario: fishing
  env:
    name: fish_baseline_concurrent
    class_name: fishing_perturbation_concurrent_env
    max_num_rounds: 12
    initial_resource_in_pool: 100
    poupulation_change_after_round: double_100_cap
    observation_other_agents_harvesting: true
    language_nature: unconstrained
    num_agents: 5
    harvesting_order: concurrent
    assign_resource_strategy: stochastic
    inject_universalization: false
    inject_scenario_dynamic: false
    perturbations: []
  agent:
    agent_package: persona_v3
    system_prompt: v3
    cot_prompt: think_step_by_step
    name: LLM=${llm.path}-S=${experiment.agent.act.harvest_strategy}-Up=${experiment.agent.act.universalization_prompt}-Id=${experiment.agent.act.consider_identity_persona}-T=${llm.temperature}-${llm.top_p}
    act:
      universalization_prompt: false
      harvest_strategy: one_step
      consider_identity_persona: true
    converse:
      inject_resource_observation: ${experiment.env.observation_other_agents_harvesting}
      inject_resource_observation_strategy: manager
      max_conversation_steps: 10
      prompt_utterance: one_shot
    store:
      expiration_delta:
        days: 63
code_version: v6.4
group_name: Meta-Llama-3-8B
llm:
  path: /gpfs/home2/overven/.cache/huggingface/hub/meta-llama--Meta-Llama-3-8B-Instruct/snapshots/5f0b02c75b57c5855da9ae460ce51323ea669d8a
  backend: transformers
  is_api: false
  render: false
  temperature: 0.0
  top_p: 1.0
seed: 100
debug: false

[2025-01-19 14:24:45,157][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:02<00:06,  2.30s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:04<00:04,  2.31s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:06<00:02,  2.28s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.61s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.86s/it]
wandb: Currently logged in as: oliver-van-erven (oliver-van-erven-university-of-amsterdam). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /gpfs/home2/overven/GovSim/wandb/run-20250119_142454-jtgihyju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-water-46
wandb: â­ï¸ View project at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS
wandb: ðŸš€ View run at https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/jtgihyju
Storage name: tough-water-46-jtgihyju
[2025-01-19 14:25:03,012][sentence_transformers.SentenceTransformer][INFO] - Load pretrained SentenceTransformer: mixedbread-ai/mxbai-embed-large-v1
/home/overven/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-01-19 14:25:04,619][sentence_transformers.SentenceTransformer][INFO] - 2 prompts are loaded, with the keys: ['query', 'passage']
/gpfs/home2/overven/GovSim/simulation/utils/models.py:176: RuntimeWarning: An exception occured: Regex \d+ not found in  N/A: Traceback (most recent call last):
  File "/gpfs/home2/overven/GovSim/simulation/utils/models.py", line 166, in find
    lm: Model = previous_lm + pathfinder.find(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/backend.py", line 99, in __add__
    res, original_res = lm._get_find(value)
                        ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home2/overven/GovSim/pathfinder/library/model.py", line 277, in _get_find
    raise Exception(f"Regex {value.regex} not found in {original_res}")
Exception: Regex \d+ not found in  N/A

Returning default value in find
  warnings.warn(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
/gpfs/home2/overven/GovSim/simulation/scenarios/common/environment/concurrent_env.py:239: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  pd.concat(self.df_acc).to_json(
wandb: Adding directory to artifact (/gpfs/home2/overven/GovSim/simulation/results/fishing_v6.4/Meta-Llama-3-8B/tough-water-46/.hydra)... Done. 0.0s
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.097 MB uploadedwandb: - 0.097 MB of 0.097 MB uploadedwandb: 
wandb: Run history:
wandb:     conversation_resource_limit â–
wandb:                  experiment/TFS â–â–â–â–â–„â–…â–…â–†â–†â–†â–†â–„â–†â–‡â–‡â–‡â–‡â–†â–†â–…â–â–ƒâ–‚â–‚â–†â–†â–…â–‡â–‡â–‡â–‡â–ƒâ–ƒâ–†â–…â–†â–†â–ˆâ–ˆâ–‡
wandb:       experiment/TFS_cumulative â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  experiment/token_in_cumulative â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆ
wandb: experiment/token_out_cumulative â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb:                    num_resource â–ˆâ–ˆâ–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:    persona_0_collected_resource â–ˆâ–
wandb:    persona_1_collected_resource â–ˆâ–
wandb:    persona_2_collected_resource â–ˆâ–
wandb:    persona_3_collected_resource â–ˆâ–
wandb:    persona_4_collected_resource â–ˆâ–
wandb: 
wandb: Run summary:
wandb:                  experiment/TFS 783.74366
wandb:       experiment/TFS_cumulative 517.85491
wandb:  experiment/token_in_cumulative 183476
wandb: experiment/token_out_cumulative 8647
wandb:                    num_resource 0
wandb:    persona_0_collected_resource 8
wandb:    persona_1_collected_resource 8
wandb:    persona_2_collected_resource 8
wandb:    persona_3_collected_resource 8
wandb:    persona_4_collected_resource 6
wandb: 
wandb: ðŸš€ View run tough-water-46 at: https://wandb.ai/oliver-van-erven-university-of-amsterdam/EMS/runs/jtgihyju
wandb: Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250119_142454-jtgihyju/logs

JOB STATISTICS
==============
Job ID: 9495071
Cluster: snellius
User/Group: overven/overven
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 9
CPU Utilized: 00:24:57
CPU Efficiency: 20.16% of 02:03:45 core-walltime
Job Wall-clock time: 00:13:45
Memory Utilized: 4.23 GB
Memory Efficiency: 7.05% of 60.00 GB
